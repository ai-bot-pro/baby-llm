## tinystories
- download from hg roneneldan/TinyStories TinyStories datasets 
paper: [TinyStories: How Small Can Language Models Be and Still Speak Coherent English?](https://arxiv.org/pdf/2305.07759.pdf)
> TinyStories, a synthetic dataset of short stories that only contain words that a typical
3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4.

> We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource
or specialized domains, and shed light on the emergence of language capabilities in LMs. A general question that
arises from this work is whether synthesizing a refined dataset can be beneficial in training networks for practical
uses. For example, **perhaps it is possible to train a customer service chatbot by synthesizing a large dataset of hypothetical calls**.

- 使用[deep-translator](https://github.com/nidhaloff/deep-translator)翻译的[51AI/TinyStoriesZh](https://huggingface.co/datasets/52AI/TinyStoriesZh), 翻译GPT生成的story部分，探索在中文数据集上小模型Speak Coherent Chinese的能力。 


```shell
# 1. download tinystories
bash ./llama2/datasets/tinystories/download.sh

# 2. if need train tokenizer, train tokenizer vocab size=8196
python3 ./llama2/datasets/tinystories/prepocess.py train_vocab --vocab_size=8192 --data_dir=${data_dir}

# 3. if need merge, merge trained tokenizer to src tokenizer
python3 ./llama2/datasets/tinystories/prepocess.py merge_tokenizer --data_dir=${data_dir} --src_tokenizer_model=${src} --merge_tokenizer_model=${merge}

# 4. use trained tokenizer vocab model pretokenize tinystories datasets to tokenizer id for model trainning and inference
python3 ./llama2/datasets/tinystories/prepocess.py pretokenize --vocab_size=8192 --data_dir=${data_dir} --tokenizer_model=${model}

# 5. export pb tokenizer model to binary file for LM training and inference if use sentencepiece
python3 ./llama2/datasets/tinystories/tokenizer.py --tokenizer-model=${model}
```