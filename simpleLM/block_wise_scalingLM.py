r"""
This scaling is known as layer-wise or block-wise scaling from:
[DELIGHT: DEEP AND LIGHT-WEIGHT TRANSFORMER](https://arxiv.org/abs/2008.00623)
- attention layers
- FFN layers
reference: https://github.com/apple/corenet/blob/main/corenet/modeling/models/language_modeling/general_gpt.py
"""
