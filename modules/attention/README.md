# intro
encoder-decoder for seq2seq with attention

# reference
- https://en.wikipedia.org/wiki/Attention_(machine_learning)
- [Recurrent Models of Visual Attention](https://arxiv.org/abs/1406.6247) (RNN(LSTM)+attention)
- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)(LSTM+Additive Attention)
- [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044) (LSTM+(hard/soft)attetion)
- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)(LSTM+(global/local)multiplicative attention)
- ❤[**Attention Is All You Need**](https://arxiv.org/abs/1706.03762)❤ (Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. self attention(Scaled Dot-Product Attention(SDPA), Multi-Head Attention))