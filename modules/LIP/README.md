# LIP(Language Image Pre-Trainning)
a neural network trained on a variety of (image, text) pairs to embedding for text-image task(image-text similarity retrieval and for zero-shot image classification). more use in VLM

## CLIP
- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) (CLIP: Contrastive Language-Image Pre-Training) (CLIP-ResNet/ViT)
- https://github.com/openai/CLIP
- https://huggingface.co/docs/transformers/model_doc/clip
- [Reproducible scaling laws for contrastive language-image learning](https://arxiv.org/abs/2212.07143)
- https://github.com/LAION-AI/scaling-laws-openclip
- https://github.com/mlfoundations/open_clip


## SigLIP
- [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/abs/2303.15343) (SigLIP-ViT)
- https://github.com/google-research/big_vision/blob/main/big_vision/trainers/proj/image_text/siglip.py
- https://huggingface.co/docs/transformers/model_doc/siglip

